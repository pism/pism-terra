{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d539dab6-c4b2-4fc7-aa50-49cc2e485f2c",
   "metadata": {},
   "source": [
    "# PISM TERRA for AGU 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be45f012-7926-4cd3-8cd2-8c3317cdd633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import matplotlib.pylab as plt\n",
    "import re\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "import requests\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fe5ee-6442-4c70-9893-c91bf16cb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_config(\n",
    "    ds,\n",
    "    regexp: str = \"id_(.+?)_\",\n",
    "    dim: str = \"exp_id\",\n",
    "    drop_vars: list[str] | None = None,\n",
    "    drop_dims: list[str] = [\"nv4\"],\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Add experiment identifier to the dataset.\n",
    "\n",
    "    This function processes the dataset by extracting an experiment identifier from the filename\n",
    "    using a regular expression, adding it as a new dimension, and optionally dropping specified\n",
    "    variables and dimensions from the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray.Dataset\n",
    "        The input dataset to be processed.\n",
    "    regexp : str, optional\n",
    "        The regular expression pattern to extract the experiment identifier from the filename, by default \"id_(.+?)_\".\n",
    "    dim : str, optional\n",
    "        The name of the new dimension to be added to the dataset, by default \"exp_id\".\n",
    "    drop_vars : list[str]| None, optional\n",
    "        A list of variable names to be dropped from the dataset, by default None.\n",
    "    drop_dims : list[str], optional\n",
    "        A list of dimension names to be dropped from the dataset, by default [\"nv4\"].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "        The processed dataset with the experiment identifier added as a new dimension, and specified variables and dimensions dropped.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        If the regular expression does not match any part of the filename.\n",
    "    \"\"\"\n",
    "\n",
    "    if dim not in ds.dims:\n",
    "        m_id_re = re.search(regexp, ds.encoding[\"source\"])\n",
    "        ds = ds.expand_dims(dim)\n",
    "        assert m_id_re is not None\n",
    "        m_id: str | int\n",
    "        try:\n",
    "            m_id = int(m_id_re.group(1))\n",
    "        except:\n",
    "            m_id = str(m_id_re.group(1))\n",
    "        ds[dim] = [m_id]\n",
    "\n",
    "    p_config = ds[\"pism_config\"]\n",
    "\n",
    "    # List of suffixes to exclude\n",
    "    suffixes_to_exclude = [\"_doc\", \"_type\", \"_units\", \"_option\", \"_choices\"]\n",
    "\n",
    "    # Filter the dictionary\n",
    "    config = {k: v for k, v in p_config.attrs.items() if not any(k.endswith(suffix) for suffix in suffixes_to_exclude)}\n",
    "    if \"geometry.front_retreat.prescribed.file\" not in config.keys():\n",
    "        config[\"geometry.front_retreat.prescribed.file\"] = \"false\"\n",
    "\n",
    "    config_sorted = OrderedDict(sorted(config.items()))\n",
    "\n",
    "    pc_keys = np.array(list(config_sorted.keys()))\n",
    "    pc_vals = np.array(list(config_sorted.values()))\n",
    "\n",
    "    pism_config = xr.DataArray(\n",
    "        pc_vals.reshape(-1, 1),\n",
    "        dims=[\"pism_config_axis\", dim],\n",
    "        coords={\"pism_config_axis\": pc_keys, dim: [m_id]},\n",
    "        name=\"pism_config\",\n",
    "    )\n",
    "    ds = xr.merge(\n",
    "        [\n",
    "            ds.drop_vars([\"pism_config\", \"run_stats\"], errors=\"ignore\").drop_dims(\n",
    "                [\"pism_config_axis\", \"run_stats_axis\"], errors=\"ignore\"\n",
    "            ),\n",
    "            pism_config,\n",
    "        ]\n",
    "    )\n",
    "    return ds.drop_vars(drop_vars, errors=\"ignore\").drop_dims(drop_dims, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def pick(files, pattern):\n",
    "    for f in files:\n",
    "        if pattern in Path(f).name:\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "@xr.register_dataset_accessor(\"utils\")\n",
    "class UtilsMethods:\n",
    "    \"\"\"\n",
    "    Utils methods for xarray Dataset.\n",
    "\n",
    "    This class is used to add custom methods to xarray Dataset objects. The methods can be accessed via the 'utils' attribute.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xarray_obj : xr.Dataset\n",
    "        The xarray Dataset to which to add the custom methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, xarray_obj: xr.Dataset):\n",
    "        \"\"\"\n",
    "        Initialize the UtilsMethods class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xarray_obj : xr.Dataset\n",
    "            The xarray Dataset to which to add the custom methods.\n",
    "        \"\"\"\n",
    "        self._obj = xarray_obj\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\"\n",
    "        Do-nothing method.\n",
    "\n",
    "        This method is needed to work with joblib Parallel.\n",
    "        \"\"\"\n",
    "\n",
    "    def drop_nonnumeric_vars(self, errors: str = \"ignore\") -> xr.Dataset:\n",
    "        \"\"\"\n",
    "        Drop non-numeric variables from the xarray Dataset.\n",
    "\n",
    "        This method removes all variables from the xarray Dataset that do not have a numeric data type.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        errors : {'ignore', 'raise'}, optional\n",
    "            If 'ignore', suppress error and only drop existing variables.\n",
    "            If 'raise', raise an error if any of the variables are not found in the dataset.\n",
    "            Default is 'ignore'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        xarray.Dataset\n",
    "            A new xarray Dataset with only numeric variables.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> import xarray as xr\n",
    "        >>> import numpy as np\n",
    "        >>> data = xr.Dataset({\n",
    "        ...     'temperature': (('x', 'y'), [[15.5, 16.2], [14.8, 15.1]]),\n",
    "        ...     'humidity': (('x', 'y'), [[80, 85], [78, 82]]),\n",
    "        ...     'location': (('x', 'y'), [['A', 'B'], ['C', 'D']])\n",
    "        ... })\n",
    "        >>> processor = DataProcessor(data)\n",
    "        >>> numeric_data = processor.drop_nonnumeric_vars()\n",
    "        >>> print(numeric_data)\n",
    "        <xarray.Dataset>\n",
    "        Dimensions:     (x: 2, y: 2)\n",
    "        Dimensions without coordinates: x, y\n",
    "        Data variables:\n",
    "            temperature  (x, y) float64 15.5 16.2 14.8 15.1\n",
    "            humidity     (x, y) int64 80 85 78 82\n",
    "        \"\"\"\n",
    "        nonnumeric_vars = [v for v in self._obj.data_vars if not np.issubdtype(self._obj[v].dtype, np.number)]\n",
    "\n",
    "        return self._obj.drop_vars(nonnumeric_vars, errors=errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ca8e9-75c7-4eb8-9e61-e329a77a6e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = [0.05, 0.95]\n",
    "percentile_range = (percentiles[1] - percentiles[0]) * 100\n",
    "\n",
    "fontsize = 6\n",
    "\n",
    "sim_alpha = 0.6\n",
    "sim_cmap = [\"#CC6677\", \"#882255\"]\n",
    "obs_alpha = 1.0\n",
    "obs_cmap = [\"0.8\", \"0.9\"]\n",
    "hist_cmap = [\"#a6cee3\", \"#1f78b4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29298082-fc5b-4dbf-b221-b0fc0c7cc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyp3_sdk as sdk\n",
    "import s3fs\n",
    "from urllib.request import urlopen\n",
    "\n",
    "PISM_CLOUD_BUCKET = 'hyp3-pism-cloud-test-contentbucket-zs9dctrqrlvx'\n",
    "\n",
    "user_id = \"aaschwanden\"\n",
    "\n",
    "RGI_IDS = [\n",
    "    'RGI2000-v7.0-C-01-09429',  # Malaspina\n",
    "    'RGI2000-v7.0-C-01-04374',  # Wrangle Mountains\n",
    "    'RGI2000-v7.0-C-01-14907',  # TBD\n",
    "]\n",
    "\n",
    "campaign = \"_era5_ec2_1year\"\n",
    "JOB_NAMES = [rgi_id + campaign for rgi_id in RGI_IDS]\n",
    "\n",
    "hyp3 = sdk.HyP3('https://pism-cloud-test.asf.alaska.edu')\n",
    "jobs = sdk.Batch()\n",
    "for job_name in JOB_NAMES:\n",
    "    jobs += hyp3.find_jobs(name=job_name, user_id=user_id, job_type='PISM_TERRA_EXECUTE')\n",
    "\n",
    "\n",
    "s3_ids = {}\n",
    "for job in jobs:\n",
    "    if job.job_type == 'PISM_TERRA_EXECUTE':\n",
    "        rgi_id = job.name.split(campaign)[0]\n",
    "        print(rgi_id)\n",
    "        print(\"-\" * 80)\n",
    "        print(job.status_code)\n",
    "        if job.status_code == \"SUCCEEDED\":\n",
    "            s3_id = f's3://{PISM_CLOUD_BUCKET}/{job.job_id}/'\n",
    "            s3_ids[rgi_id] = s3_id\n",
    "        # if job.logs is not None and len(job.logs) > 0:\n",
    "        #     url = job.logs[0]\n",
    "        #     with urlopen(url) as f:\n",
    "        #         print(f.read().decode().splitlines()[-1])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe4329f-985c-462f-bfd5-fe504ffc64e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d6db3c-4eb6-406c-b4dd-eef08c370686",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(anon=True)  # or anon=True if public\n",
    "\n",
    "scalar_files = []\n",
    "spatial_files = []\n",
    "\n",
    "for rgi_id, s3_id in s3_ids.items():\n",
    "    print(rgi_id, s3_id)\n",
    "    prefix = f\"{s3_id}{rgi_id}/output/spatial/\"\n",
    "    files = fs.ls(prefix)  # returns 'bucket/key' style strings\n",
    "\n",
    "    spatial_file = pick(files, \"clipped_spatial_\")\n",
    "    scalar_file  = pick(files, \"fldsum_spatial_\")\n",
    "\n",
    "    if spatial_file is not None:\n",
    "        spatial_files.append(f\"s3://{spatial_file}\")\n",
    "    if scalar_file is not None:\n",
    "        scalar_files.append(f\"s3://{scalar_file}\")\n",
    "\n",
    "\n",
    "scalar_ds = xr.open_mfdataset(scalar_files, preprocess=partial(preprocess_config, drop_vars=[\"wall_clock_time\"]), parallel=True, engine=\"h5netcdf\")\n",
    "normalized_ds = scalar_ds.utils.drop_nonnumeric_vars() - scalar_ds.isel({\"time\": 0}).utils.drop_nonnumeric_vars()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", r\"All-NaN (slice|axis) encountered\")\n",
    "    sim_quantiles = {}\n",
    "    for q in [percentiles[0], 0.5, percentiles[1]]:\n",
    "        sim_quantiles[q] = normalized_ds.quantile(q, dim=\"exp_id\", skipna=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8c678-d808-4923-b5d7-465db81ba970",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_median = True\n",
    "with mpl.rc_context({\"font.size\": fontsize}):\n",
    "\n",
    "    p_var = \"thk\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6.4, 3.2))\n",
    "    sim_cis = []\n",
    "    sim_ci = ax.fill_between(\n",
    "        sim_quantiles[0.5].time,\n",
    "        sim_quantiles[percentiles[0]][p_var],\n",
    "        sim_quantiles[percentiles[1]][p_var],\n",
    "        alpha=sim_alpha,\n",
    "        color=sim_cmap[0],\n",
    "        lw=0,\n",
    "        label=f\"\"\"({percentile_range:.0f}% credibility interval)\"\"\",\n",
    "    )\n",
    "    sim_cis.append(sim_ci)\n",
    "    \n",
    "    if add_median:\n",
    "        sim_quantiles[0.5][p_var].plot(\n",
    "            color=sim_cmap[1],\n",
    "            add_legend=False,\n",
    "            ax=ax,\n",
    "            lw=1,\n",
    "            ls=\"solid\",\n",
    "        )\n",
    "    l = ax.legend(handles=sim_cis, loc=\"lower left\")\n",
    "    l.get_frame().set_linewidth(0.0)\n",
    "    l.get_frame().set_alpha(0.0)\n",
    "    ax.set_title(rgi_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c9c5e-cd9a-4a8b-8026-23b5f248eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92933479-bff6-4e6a-ad29-dd275835698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from moviepy import ImageClip, CompositeVideoClip\n",
    "from moviepy.video import fx as vfx\n",
    "\n",
    "base = Path(\"/Users/andy\") / \"Google Drive\" / \"My Drive\" / \"Projects\" / \"terra\"\n",
    "\n",
    "BG_FILE    = base / \"figures\" / \"ak_dem_8x.png\"\n",
    "CLOUD_FILE = base / \"figures\" / \"clouds.jpg\"\n",
    "LOGO_FILE  = base / \"figures\" / \"pism_logo.png\"\n",
    "\n",
    "duration    = 10.0      # total video length (s)\n",
    "\n",
    "cloud_start = 0.5       # when clouds start appearing\n",
    "cloud_in    = 2.0       # cloud fade-in duration\n",
    "cloud_hold  = 4.0       # how long clouds stay fully visible\n",
    "cloud_out   = 3.0       # cloud fade-out duration\n",
    "\n",
    "logo_start  = 2.2       # when logo starts to appear\n",
    "logo_in     = 1.5       # logo fade-in duration\n",
    "\n",
    "# ---- background ----\n",
    "bg = ImageClip(str(BG_FILE)).with_duration(duration)\n",
    "\n",
    "# Force even width/height for H.264 / yuv420p\n",
    "w_even = bg.w if bg.w % 2 == 0 else bg.w - 1\n",
    "h_even = bg.h if bg.h % 2 == 0 else bg.h - 1\n",
    "bg = bg.resized(new_size=(w_even, h_even))\n",
    "\n",
    "# ---- clouds layer ----\n",
    "cloud = (\n",
    "    ImageClip(str(CLOUD_FILE))\n",
    "    .resized(width=bg.w * 1.3)      # a bit larger than frame\n",
    "    .with_duration(cloud_in + cloud_hold + cloud_out)\n",
    "    .with_start(cloud_start)        # appear over the DEM\n",
    "    .with_opacity(0.9)\n",
    ")\n",
    "\n",
    "def cloud_pos(t):\n",
    "    # t is local time since cloud_start\n",
    "    x = bg.w / 2 + 60 * (t / cloud.duration)   # drift slightly right\n",
    "    y = bg.h / 2 - 20 * (t / cloud.duration)   # and slightly up\n",
    "    return (x - cloud.w / 2, y - cloud.h / 2)\n",
    "\n",
    "cloud = cloud.with_position(cloud_pos)\n",
    "cloud = cloud.with_effects([vfx.CrossFadeIn(cloud_in), vfx.CrossFadeOut(cloud_out)])\n",
    "\n",
    "# ---- logo base clip (no position yet) ----\n",
    "logo_base = (\n",
    "    ImageClip(str(LOGO_FILE))\n",
    "    .resized(height=int(bg.h * 0.15))          # scale relative to frame\n",
    "    .with_duration(duration - logo_start)\n",
    "    .with_start(logo_start)\n",
    ")\n",
    "\n",
    "# Where you want the logo on screen\n",
    "logo_pos = (100, 100)\n",
    "\n",
    "# ---- shadow for logo ----\n",
    "shadow_offset = (8, 8)  # pixels (x, y) offset of the shadow\n",
    "\n",
    "shadow = (\n",
    "    logo_base\n",
    "    .resized(height=int(150))          # scale relative to frame\n",
    "    .with_position((logo_pos[0] + shadow_offset[0],\n",
    "                    logo_pos[1] + shadow_offset[1]))\n",
    "    .with_opacity(0.4)                    # semi-transparent\n",
    "    .with_effects([vfx.CrossFadeIn(logo_in)])  # fade in with the logo\n",
    ")\n",
    "\n",
    "# ---- actual logo layer (on top of shadow) ----\n",
    "logo = (\n",
    "    logo_base\n",
    "    .resized(height=int(150))          # scale relative to frame\n",
    "    .with_position(logo_pos)\n",
    "    .with_effects([vfx.CrossFadeIn(logo_in)])  # rise out of cloud\n",
    ")\n",
    "\n",
    "# ---- composite & export ----\n",
    "final = CompositeVideoClip([bg, cloud, shadow, logo], size=bg.size)\n",
    "\n",
    "final.write_videofile(\n",
    "    base / \"animation\" / \"ak_cloud_logo.mp4\",\n",
    "    fps=30,\n",
    "    codec=\"libx264\",\n",
    "    audio=False,\n",
    "    ffmpeg_params=[\"-pix_fmt\", \"yuv420p\"],\n",
    ")\n",
    "\n",
    "# last frame as PNG (assuming fps is defined)\n",
    "fps = 30\n",
    "final.save_frame(\n",
    "    base / \"animation\" / \"ak_cloud_logo.png\",\n",
    "    t=final.duration - 1.0 / fps,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b2612-069a-4298-8554-a9dfc9dbf1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg.resized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a7717-f175-4ae4-8f78-ee1bfeecc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlopen(\"s3://pism-cloud-data/terra/era5_ec2_1year.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65989682-a301-4cee-b1b0-64eb51e992ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import hyp3_sdk as sdk\n",
    "import s3fs\n",
    "\n",
    "\n",
    "PISM_CLOUD_BUCKET = 'hyp3-pism-cloud-test-contentbucket-zs9dctrqrlvx'\n",
    "\n",
    "STAGE_TEMPLATE =     {\n",
    "    # \"name\": \"RGI2000-v7.0-C-01-09429_era5_agu_1year\",\n",
    "    \"job_type\": \"PISM_TERRA_PREP_ENSEMBLE\",\n",
    "    \"job_parameters\": {\n",
    "        # \"rgi_id\": \"RGI2000-v7.0-C-01-09429\",\n",
    "        \"rgi_gpkg\": \"s3://pism-cloud-data/terra/rgi.gpkg\",\n",
    "        \"pism_config\": \"s3://pism-cloud-data/terra/era5_ec2_1year.toml\",\n",
    "        \"run_template\": \"s3://pism-cloud-data/terra/ec2.j2\",\n",
    "        \"uq_config\": \"s3://pism-cloud-data/terra/era5_agu.toml\"\n",
    "    }\n",
    "}\n",
    "\n",
    "EXECUTE_TEMPLATE = {\n",
    "    # \"name\": \"RGI2000-v7.0-C-01-09429_era5_agu_1year\",\n",
    "    \"job_type\": \"PISM_TERRA_EXECUTE\",\n",
    "    \"job_parameters\": {\n",
    "        # \"ensemble_job_id\": \"042ffcdc-2134-4b18-b1af-b22fdf7cbb52\",\n",
    "        # \"run_script\": \"RGI2000-v7.0-C-01-09429/run_scripts/submit_g400m_RGI2000-v7.0-C-01-09429_id_0_1978-01-01_1979-01-01.sh\"\n",
    "    }\n",
    "}\n",
    "\n",
    "RGI_IDS = [\n",
    "    'RGI2000-v7.0-C-01-09429',  # Malaspina\n",
    "    'RGI2000-v7.0-C-01-04374',  # Wrangle Mountains\n",
    "    'RGI2000-v7.0-C-01-14907',  # TBD\n",
    "]\n",
    "\n",
    "\n",
    "def get_run_scripts(job: sdk.Job) ->  list[str]:\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    files = fs.ls(f'{PISM_CLOUD_BUCKET}/{job.job_id}/{job.job_parameters[\"rgi_id\"]}/run_scripts')\n",
    "    return [str(Path(file).relative_to(f'{PISM_CLOUD_BUCKET}/{job.job_id}/')) for file in files]\n",
    "\n",
    "\n",
    "hyp3 = sdk.HyP3('https://pism-cloud-test.asf.alaska.edu')\n",
    "\n",
    "# prepared_jobs = []\n",
    "# for rgi in RGI_IDS:\n",
    "#     job_dict = deepcopy(STAGE_TEMPLATE)\n",
    "#     job_dict['name'] = f'{rgi}_{Path(job_dict[\"job_parameters\"][\"pism_config\"]).stem}'\n",
    "#     job_dict['job_parameters']['rgi_id'] = rgi\n",
    "#     prepared_jobs.append(job_dict)\n",
    "\n",
    "# jobs = hyp3.submit_prepared_jobs(prepared_jobs)\n",
    "# jobs = hyp3.watch(jobs)\n",
    "\n",
    "\n",
    "# prepared_jobs = []\n",
    "# for job in jobs:\n",
    "#     run_scripts = get_run_scripts(job)\n",
    "#     for script in run_scripts:\n",
    "#         job_dict = deepcopy(EXECUTE_TEMPLATE)\n",
    "#         job_dict['name'] = job.name\n",
    "#         job_dict['job_parameters']['ensemble_job_id'] = job.job_id\n",
    "#         job_dict['job_parameters']['run_script'] = script\n",
    "#         prepared_jobs.append(job_dict)\n",
    "\n",
    "# jobs += hyp3.submit_prepared_jobs(prepared_jobs)\n",
    "# jobs = hyp3.watch(jobs)\n",
    "\n",
    "# job_names = {job.name for job in jobs}\n",
    "# print(job_names)\n",
    "\n",
    "hyp3 = sdk.HyP3('https://pism-cloud-test.asf.alaska.edu')\n",
    "jobs = sdk.Batch()\n",
    "for job_name in {'RGI2000-v7.0-C-01-14907_era5_ec2_1year', 'RGI2000-v7.0-C-01-09429_era5_ec2_1year', 'RGI2000-v7.0-C-01-04374_era5_ec2_1year'}:\n",
    "    jobs += hyp3.find_jobs(name=job_name, user_id='aaschwanden', job_type='PISM_TERRA_EXECUTE')\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "jobs = hyp3.watch(jobs)\n",
    "\n",
    "print('Path to job files:')\n",
    "for job in jobs:\n",
    "    if job.job_type == 'PISM_TERRA_EXECUTE':\n",
    "        print(f'{job.name}: s3://{PISM_CLOUD_BUCKET}/{job.job_id}/')\n",
    "        print(f'    Run Script: {job.job_parameters[\"run_script\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544f487-c2ce-45c1-8ecc-d6af570a1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9842554d-31bf-4d56-b500-1eed4a0561b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp3 = sdk.HyP3('https://pism-cloud-test.asf.alaska.edu')\n",
    "jobs = sdk.Batch()\n",
    "for job_name in {'RGI2000-v7.0-C-01-14907_era5_ec2_1year', 'RGI2000-v7.0-C-01-09429_era5_ec2_1year', 'RGI2000-v7.0-C-01-04374_era5_ec2_1year'}:\n",
    "    jobs += hyp3.find_jobs(name=job_name, user_id='aaschwanden', job_type='PISM_TERRA_EXECUTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267daaa-d9aa-4fbb-bd57-4fb8f281b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Path to job files:')\n",
    "for job in jobs:\n",
    "    if job.job_type == 'PISM_TERRA_EXECUTE':\n",
    "        print(f'{job.name}: s3://{PISM_CLOUD_BUCKET}/{job.job_id}/')\n",
    "        print(f'    Run Script: {job.job_parameters[\"run_script\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba60a49-c9a2-4c78-8df3-8eed29f37881",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653511d6-1c84-4bf0-8ed5-7b4ccaea0e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824660ad-f16d-40b0-b3e4-1a10a734e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "with fs.open(\"s3://pism-cloud-data/terra/era5_ec2_1year.toml\", \"r\") as f:\n",
    "    last = None\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        last = line.rstrip()\n",
    "\n",
    "print(last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f56427-b491-4453-bb62-31c2b4d0f375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
